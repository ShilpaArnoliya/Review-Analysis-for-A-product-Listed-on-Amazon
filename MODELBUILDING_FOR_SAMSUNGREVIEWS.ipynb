{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4929 entries, 0 to 4999\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   stars        4929 non-null   float64\n",
      " 1   text         4929 non-null   object \n",
      " 2   title        4929 non-null   object \n",
      " 3   review_date  4929 non-null   object \n",
      "dtypes: float64(1), object(3)\n",
      "memory usage: 192.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\DELL\\\\allreviews_samsung.csv\")\n",
    "df = df.drop_duplicates()\n",
    "df = df.rename(columns ={\"comment\": \"text\"})                             \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['token_length'] = [len(x.split(\" \")) for x in df.text]\n",
    "max(df.token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n    \\n  Capacity and working was goodThe packaging was really good even for a memory card which one can easily misunderstand for a mobile or similar stuff visuall.However there are a lot of fake ones available so in order to find out which one is real and which is fake check the sides of the memory card.The real ones edges will be white in colour while the fake ones have black coloured edges.Fast data transferring Adapter is include with this card.Pros:-1- The read/write speed of the card through the adapter is around 50/25 mbps.2- The read/write speed of the card through my mobile ZenFone 2 ZE550ML variant with the original cable is around 40/28 mbps well technically it is more than what we get through the original adapter.3- The card really is water proof since I accidentally dropped a glass of water on it and it was still working fine without any problems.4-This card is really fast so fast that I have like 40 movies on my mobile(literally and I am not joking) 10 GB of video songs and a few pictures and stuff and even with only 300 MB of free space the memory card never hangs.5- I usually carry a lot of movies and compressed RAR files I have like roughly 900 files on my memory card right now and I was able to copy around 40 GB of this data using my mobile cable not THROUGH a adapter in 30 minutes give or take 3 minutes from my pc to the card.6- I copied the same data back to my pc using the same setup and was able to copy them all in comically the same amount of time.160 people found this helpful\\n\\n  \\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.token_length.idxmax(),'text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \n",
    "                   \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
    "                   \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\"didn't\": \"did not\", \n",
    "                   \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
    "                   \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
    "                   \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \n",
    "                   \"he'll've\": \"he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
    "                   \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \n",
    "                   \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n",
    "                   \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \n",
    "                   \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \n",
    "                   \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n",
    "                   \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \n",
    "                   \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \n",
    "                   \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n",
    "                   \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
    "                   \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n",
    "                   \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n",
    "                   \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "                   \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n",
    "                   \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
    "                   \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n",
    "                   \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \n",
    "                   \"this's\": \"this is\",\n",
    "                   \"that'd\": \"that would\", \"that'd've\": \"that would have\",\"that's\": \"that is\", \n",
    "                   \"there'd\": \"there would\", \"there'd've\": \"there would have\",\"there's\": \"there is\", \n",
    "                       \"here's\": \"here is\",\n",
    "                   \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n",
    "                   \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n",
    "                   \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n",
    "                   \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n",
    "                   \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \n",
    "                   \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \n",
    "                   \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n",
    "                   \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n",
    "                   \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n",
    "                   \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \n",
    "                   \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n",
    "                   \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n",
    "                   \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                   \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                   \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n",
    "                   \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import unidecode\n",
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def spacy_cleaner(text):\n",
    "    try:\n",
    "        decoded = unidecode.unidecode(codecs.decode(text, 'unicode_escape'))\n",
    "    except:\n",
    "        decoded = unidecode.unidecode(text)\n",
    "    apostrophe_handled = re.sub(\"â€™\", \"'\", decoded)\n",
    "    expanded = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in apostrophe_handled.split(\" \")])\n",
    "    parsed = nlp(expanded)\n",
    "    final_tokens = []\n",
    "    for t in parsed:\n",
    "        if t.is_punct or t.is_space or t.like_num or t.like_url or str(t).startswith('@'):\n",
    "            pass\n",
    "        else:\n",
    "            if t.lemma_ == '-PRON-':\n",
    "                final_tokens.append(str(t))\n",
    "            else:\n",
    "                sc_removed = re.sub(\"[^a-zA-Z]\", '', str(t.lemma_))\n",
    "                if len(sc_removed) > 1:\n",
    "                    final_tokens.append(sc_removed)\n",
    "    joined = ' '.join(final_tokens)\n",
    "    spell_corrected = re.sub(r'(.)\\1+', r'\\1\\1', joined)\n",
    "    return spell_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text'] = [spacy_cleaner(t) for t in df.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define sentiment class\n",
    "# decide sentiment as positive, negative and neutral \n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos    3766\n",
       "neu     788\n",
       "neg     375\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sentiment_analyzer_scores(sentence):\n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "    score = analyser.polarity_scores(sentence)\n",
    "    if(score['compound']>=0.2):\n",
    "        return 'pos'\n",
    "    elif(score['compound']<=-0.2):\n",
    "        return 'neg'\n",
    "    else:\n",
    "        return 'neu'\n",
    "  \n",
    "\n",
    "df['sentiment'] = df['clean_text'].apply(lambda x: sentiment_analyzer_scores(x))   \n",
    "\n",
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=5000, min_df=5, max_df=0.7, stop_words=None)\n",
    "X = vectorizer.fit_transform(df.clean_text).toarray()\n",
    "#from sklearn.model_selection import StratifiedKFold\n",
    "#from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidfconverter = TfidfTransformer()\n",
    "X = tfidfconverter.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3943, 832) (3943,)\n",
      "(986, 832) (986,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df.sentiment, test_size=0.2, random_state=0)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.09      0.56      0.16        70\n",
      "         neu       0.21      0.37      0.27       170\n",
      "         pos       0.80      0.28      0.42       746\n",
      "\n",
      "    accuracy                           0.32       986\n",
      "   macro avg       0.37      0.40      0.28       986\n",
      "weighted avg       0.65      0.32      0.37       986\n",
      "\n",
      "[[ 39  10  21]\n",
      " [ 75  63  32]\n",
      " [314 222 210]]\n",
      " accuracy =  0.31643002028397565\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "sgnb = GaussianNB()\n",
    "spred_gnb = sgnb.fit(X_train,y_train).predict(X_test)\n",
    "# summarize the fit of the model\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, spred_gnb))\n",
    "print(metrics.confusion_matrix(y_test, spred_gnb))\n",
    "print(\" accuracy = \", accuracy_score(y_test, spred_gnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.10      0.56      0.16        70\n",
      "         neu       0.26      0.42      0.32       170\n",
      "         pos       0.82      0.34      0.48       746\n",
      "\n",
      "    accuracy                           0.37       986\n",
      "   macro avg       0.39      0.44      0.32       986\n",
      "weighted avg       0.67      0.37      0.43       986\n",
      "\n",
      "[[ 39  10  21]\n",
      " [ 63  71  36]\n",
      " [302 191 253]]\n",
      " accuracy =  0.36815415821501013\n"
     ]
    }
   ],
   "source": [
    "################                   dealing with imbalanced data             ###################\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state = 2) \n",
    "X_train_res, y_train_res = sm.fit_sample(X_train,np.array(y_train).ravel())\n",
    "\n",
    "###  building NB model after applying SMOTE \n",
    "smote_gnb = sgnb.fit(X_train_res,y_train_res).predict(X_test)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(y_test, smote_gnb))\n",
    "print(metrics.confusion_matrix(y_test, smote_gnb))\n",
    "print(\" accuracy = \", accuracy_score(y_test, smote_gnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.85      0.47      0.61        70\n",
      "         neu       0.76      0.49      0.60       170\n",
      "         pos       0.88      0.98      0.93       746\n",
      "\n",
      "    accuracy                           0.86       986\n",
      "   macro avg       0.83      0.65      0.71       986\n",
      "weighted avg       0.85      0.86      0.85       986\n",
      "\n",
      "[[ 33  14  23]\n",
      " [  5  84  81]\n",
      " [  1  13 732]]\n",
      " accuracy =  0.8610547667342799\n"
     ]
    }
   ],
   "source": [
    ">>> from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "rf_pred = rf.fit(X_train, y_train).predict(X_test)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(y_test, rf_pred))\n",
    "print(metrics.confusion_matrix(y_test, rf_pred))\n",
    "print(\" accuracy = \", accuracy_score(y_test, rf_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.43      0.56      0.48        70\n",
      "         neu       0.65      0.64      0.65       170\n",
      "         pos       0.93      0.91      0.92       746\n",
      "\n",
      "    accuracy                           0.84       986\n",
      "   macro avg       0.67      0.70      0.68       986\n",
      "weighted avg       0.85      0.84      0.84       986\n",
      "\n",
      "[[ 39  21  10]\n",
      " [ 23 109  38]\n",
      " [ 29  37 680]]\n",
      " accuracy =  0.8397565922920892\n"
     ]
    }
   ],
   "source": [
    "################                   dealing with imbalanced data             ###################\n",
    "###  building RF model after applying SMOTE \n",
    "smote_rf = rf.fit(X_train_res,y_train_res).predict(X_test)\n",
    "# summarize the fit of the model\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, smote_rf))\n",
    "print(metrics.confusion_matrix(y_test, smote_rf))\n",
    "print(\" accuracy = \", accuracy_score(y_test, smote_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.63      0.51      0.57        70\n",
      "         neu       0.72      0.65      0.68       170\n",
      "         pos       0.93      0.96      0.94       746\n",
      "\n",
      "    accuracy                           0.88       986\n",
      "   macro avg       0.76      0.71      0.73       986\n",
      "weighted avg       0.87      0.88      0.87       986\n",
      "\n",
      "[[ 36  19  15]\n",
      " [ 18 111  41]\n",
      " [  3  25 718]]\n",
      " accuracy =  0.8772819472616633\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM_pred = SVM.fit(X_train, y_train).predict(X_test)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(y_test, SVM_pred))\n",
    "print(metrics.confusion_matrix(y_test, SVM_pred))\n",
    "print(\" accuracy = \", accuracy_score(y_test, SVM_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.44      0.71      0.54        70\n",
      "         neu       0.68      0.72      0.70       170\n",
      "         pos       0.98      0.90      0.94       746\n",
      "\n",
      "    accuracy                           0.86       986\n",
      "   macro avg       0.70      0.78      0.73       986\n",
      "weighted avg       0.89      0.86      0.87       986\n",
      "\n",
      "[[ 50  16   4]\n",
      " [ 34 123  13]\n",
      " [ 30  41 675]]\n",
      " accuracy =  0.8600405679513184\n"
     ]
    }
   ],
   "source": [
    "################                   dealing with imbalanced data             ###################\n",
    "###  building svm model after applying SMOTE \n",
    "smote_svm = SVM.fit(X_train_res,y_train_res).predict(X_test)\n",
    "# summarize the fit of the model\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, smote_svm))\n",
    "print(metrics.confusion_matrix(y_test, smote_svm))\n",
    "print(\" accuracy = \", accuracy_score(y_test, smote_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.62      0.34      0.44        70\n",
      "         neu       0.66      0.54      0.59       170\n",
      "         pos       0.89      0.97      0.93       746\n",
      "\n",
      "    accuracy                           0.85       986\n",
      "   macro avg       0.72      0.62      0.65       986\n",
      "weighted avg       0.83      0.85      0.84       986\n",
      "\n",
      "[[ 24  24  22]\n",
      " [ 13  92  65]\n",
      " [  2  24 720]]\n",
      " accuracy =  0.847870182555781\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr=LogisticRegression(solver=\"lbfgs\")\n",
    "# fit the training dataset on the classifier\n",
    "lr_pred = lr.fit(X_train, y_train).predict(X_test)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(y_test, lr_pred))\n",
    "print(metrics.confusion_matrix(y_test, lr_pred))\n",
    "print(\" accuracy = \", accuracy_score(y_test, lr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.39      0.70      0.50        70\n",
      "         neu       0.65      0.67      0.66       170\n",
      "         pos       0.98      0.89      0.93       746\n",
      "\n",
      "    accuracy                           0.84       986\n",
      "   macro avg       0.67      0.75      0.70       986\n",
      "weighted avg       0.88      0.84      0.86       986\n",
      "\n",
      "[[ 49  18   3]\n",
      " [ 42 114  14]\n",
      " [ 36  43 667]]\n",
      " accuracy =  0.8417849898580122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "################                   dealing with imbalanced data             ###################\n",
    "###  building lr model after applying SMOTE \n",
    "smote_lr = lr.fit(X_train_res,y_train_res).predict(X_test)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(y_test, smote_lr))\n",
    "print(metrics.confusion_matrix(y_test, smote_lr))\n",
    "print(\" accuracy = \", accuracy_score(y_test, smote_lr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
